{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rc('figure', figsize=[8,8])  #set the default figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This helper function will help you plot the decision boundary.\n",
    "# It also shows the dataset.  If you add functions to your Naive\n",
    "# Bayes classifier similar predict and fit in \n",
    "# sklearn.linear_model.LinearRegression (see the documentation)\n",
    "# use should be able to use this without any modification.  See\n",
    "# the example with LinearRegression below.\n",
    "\n",
    "def plot_decision_boundary(predict_func, X, train_func=None, y=None):\n",
    "    '''Plot decision boundary for predict_func on data X.\n",
    "    \n",
    "    If train_func and y are provided, first train the classifier.'''\n",
    "    \n",
    "    if train_func and y is not None:\n",
    "        train_func(X, y)\n",
    "        \n",
    "    # Plot decision boundary\n",
    "        \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000),\n",
    "                         np.linspace(y_min, y_max, 1000))\n",
    "    Z = predict_func(np.c_[np.around(xx.ravel()), \n",
    "                           np.around(yy.ravel())])\n",
    "    Z = np.asarray(Z).reshape(xx.shape)\n",
    "\n",
    "    _ = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.1)\n",
    "\n",
    "    # Plot data points\n",
    "    \n",
    "    # Add small random noise so points with the same values\n",
    "    # can be see differently\n",
    "    X0 = X[:,0] + np.random.random(size=len(X))/5\n",
    "    X1 = X[:,1] + np.random.random(size=len(X))/5\n",
    "\n",
    "    # Plot the two classes separately with different colors/markers\n",
    "    X0_class0 = X0[y==0] \n",
    "    X1_class0 = X1[y==0]\n",
    "    _ = plt.scatter(X0_class0, X1_class0, marker='.', c='green')\n",
    "    \n",
    "    X0_class1 = X0[y==1] \n",
    "    X1_class1 = X1[y==1]\n",
    "    _ = plt.scatter(X0_class1, X1_class1, marker='+', c='red')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "D = genfromtxt('dataset1.csv', delimiter=',')\n",
    "\n",
    "X = D[:, 0:2]\n",
    "y = D[:, 2]\n",
    "\n",
    "plot_decision_boundary(lr_clf.predict, X, lr_clf.fit, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
